{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "id": "vCp0xsCvrjJ4",
    "ExecuteTime": {
     "end_time": "2023-06-03T05:55:15.528885293Z",
     "start_time": "2023-06-03T05:55:13.626582786Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 09:55:13.797578: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-03 09:55:13.822121: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 09:55:13.964304: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-03 09:55:13.964935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 09:55:14.662023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.myWeights = tf.Variable(np.random.randn(output_size, input_size), dtype=tf.float32)\n",
    "        self.bias = tf.Variable(np.random.randn(output_size, 1), dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.myInput = inputs\n",
    "        return tf.matmul(self.myWeights, self.myInput) + self.bias\n",
    "\n",
    "    def backpropagation(self, output_gradient, learning_rate):\n",
    "        weights_gradient = tf.matmul(output_gradient, tf.transpose(self.myInput))\n",
    "        self.myWeights.assign_sub(learning_rate * weights_gradient)\n",
    "        self.bias.assign_sub(learning_rate * output_gradient)\n",
    "        return tf.matmul(tf.transpose(self.myWeights), output_gradient)"
   ],
   "metadata": {
    "id": "GUlljfZtafYo",
    "ExecuteTime": {
     "end_time": "2023-06-03T05:55:15.533189493Z",
     "start_time": "2023-06-03T05:55:15.531272273Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Activation(tf.keras.layers.Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        super(Activation, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.my_input = inputs\n",
    "        return self.activation(self.my_input)\n",
    "\n",
    "    def backpropagation(self, output_gradient, learning_rate):\n",
    "        return tf.multiply(output_gradient, self.activation_prime(self.my_input))\n"
   ],
   "metadata": {
    "id": "ZJQxHiM8chB-"
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / tf.size(y_true)\n",
    "\n"
   ],
   "metadata": {
    "id": "yCqoCVdydOTR"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return tf.math.sigmoid(x)\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super(Sigmoid, self).__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return tf.nn.relu(x)\n",
    "\n",
    "        def relu_prime(x):\n",
    "            return tf.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "        super(ReLU, self).__init__(relu, relu_prime)"
   ],
   "metadata": {
    "id": "XA_evUsAdLUC"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class DenseNetwork:\n",
    "    @staticmethod\n",
    "    def predict(network, inputs):\n",
    "        output = inputs\n",
    "        for layer in network:\n",
    "            output = layer(output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def train(network, loss, loss_prime, x_train, y_train, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        for e in range(epochs):\n",
    "            error = 0\n",
    "            for x, y in zip(x_train, y_train):\n",
    "                x = tf.convert_to_tensor(x)\n",
    "                y = tf.convert_to_tensor(y)\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    output = DenseNetwork.predict(network, x)\n",
    "                    error += loss(y, output)\n",
    "\n",
    "                grads = tape.gradient(error, [var for layer in network for var in layer.trainable_variables])\n",
    "                index = 0\n",
    "                for layer in network:\n",
    "                    layer_params = layer.trainable_variables\n",
    "                    num_params = len(layer_params)\n",
    "                    for i in range(num_params):\n",
    "                        layer_params[i].assign_sub(learning_rate * grads[index])\n",
    "                        index += 1\n",
    "\n",
    "            error /= len(x_train)\n",
    "            if verbose:\n",
    "                print(f\"{e + 1}/{epochs}, error={error}\")"
   ],
   "metadata": {
    "id": "ZyUgsolqeR4S"
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
    "X = tf.reshape(X, (4, 2, 1))\n",
    "Y = tf.constant([[0], [1], [1], [0]], dtype=tf.float32)\n",
    "Y = tf.reshape(Y, (4, 1, 1))\n",
    "\n",
    "network = [\n",
    "    DenseLayer(2, 3),\n",
    "    ReLU(),\n",
    "    DenseLayer(3, 1),\n",
    "    Sigmoid()\n",
    "]\n",
    "dense_network = DenseNetwork()\n",
    "dense_network.train(network, mse, mse_prime, X, Y, epochs=100, learning_rate=0.1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yh-9y5yZna5L",
    "outputId": "229f6432-9515-40ee-b1f6-ed6eb632b119"
   },
   "execution_count": 65,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/100, error=0.26362863183021545\n",
      "2/100, error=0.25958144664764404\n",
      "3/100, error=0.2557365596294403\n",
      "4/100, error=0.2520930767059326\n",
      "5/100, error=0.24864739179611206\n",
      "6/100, error=0.24539339542388916\n",
      "7/100, error=0.24232307076454163\n",
      "8/100, error=0.23942692577838898\n",
      "9/100, error=0.2366943359375\n",
      "10/100, error=0.23411419987678528\n",
      "11/100, error=0.23167484998703003\n",
      "12/100, error=0.229364812374115\n",
      "13/100, error=0.2271726131439209\n",
      "14/100, error=0.22508743405342102\n",
      "15/100, error=0.22309866547584534\n",
      "16/100, error=0.22119645774364471\n",
      "17/100, error=0.2193717062473297\n",
      "18/100, error=0.2176159918308258\n",
      "19/100, error=0.2159215211868286\n",
      "20/100, error=0.21428121626377106\n",
      "21/100, error=0.2126888781785965\n",
      "22/100, error=0.2111387699842453\n",
      "23/100, error=0.20962592959403992\n",
      "24/100, error=0.20814594626426697\n",
      "25/100, error=0.2066948115825653\n",
      "26/100, error=0.20526927709579468\n",
      "27/100, error=0.20402514934539795\n",
      "28/100, error=0.20281732082366943\n",
      "29/100, error=0.20242418348789215\n",
      "30/100, error=0.2009626030921936\n",
      "31/100, error=0.20041710138320923\n",
      "32/100, error=0.19920949637889862\n",
      "33/100, error=0.19812776148319244\n",
      "34/100, error=0.197921484708786\n",
      "35/100, error=0.19653311371803284\n",
      "36/100, error=0.1961037665605545\n",
      "37/100, error=0.19502946734428406\n",
      "38/100, error=0.19406847655773163\n",
      "39/100, error=0.1939033567905426\n",
      "40/100, error=0.19271141290664673\n",
      "41/100, error=0.1918235570192337\n",
      "42/100, error=0.19186076521873474\n",
      "43/100, error=0.19060483574867249\n",
      "44/100, error=0.1902812421321869\n",
      "45/100, error=0.18946021795272827\n",
      "46/100, error=0.1886812150478363\n",
      "47/100, error=0.18846395611763\n",
      "48/100, error=0.18766021728515625\n",
      "49/100, error=0.1869458705186844\n",
      "50/100, error=0.18676415085792542\n",
      "51/100, error=0.18603825569152832\n",
      "52/100, error=0.18538415431976318\n",
      "53/100, error=0.18475641310214996\n",
      "54/100, error=0.18499556183815002\n",
      "55/100, error=0.18399643898010254\n",
      "56/100, error=0.18342189490795135\n",
      "57/100, error=0.18351635336875916\n",
      "58/100, error=0.18274995684623718\n",
      "59/100, error=0.18222405016422272\n",
      "60/100, error=0.18172042071819305\n",
      "61/100, error=0.18202736973762512\n",
      "62/100, error=0.1811632513999939\n",
      "63/100, error=0.18070152401924133\n",
      "64/100, error=0.1807064414024353\n",
      "65/100, error=0.1802101731300354\n",
      "66/100, error=0.17978651821613312\n",
      "67/100, error=0.17938129603862762\n",
      "68/100, error=0.17941761016845703\n",
      "69/100, error=0.17897681891918182\n",
      "70/100, error=0.1786039024591446\n",
      "71/100, error=0.17824725806713104\n",
      "72/100, error=0.1779061108827591\n",
      "73/100, error=0.17822164297103882\n",
      "74/100, error=0.17759756743907928\n",
      "75/100, error=0.17728199064731598\n",
      "76/100, error=0.1769801378250122\n",
      "77/100, error=0.17708933353424072\n",
      "78/100, error=0.17672836780548096\n",
      "79/100, error=0.17644821107387543\n",
      "80/100, error=0.17641311883926392\n",
      "81/100, error=0.1765838861465454\n",
      "82/100, error=0.1763046681880951\n",
      "83/100, error=0.17603817582130432\n",
      "84/100, error=0.17578355967998505\n",
      "85/100, error=0.1755402386188507\n",
      "86/100, error=0.17530757188796997\n",
      "87/100, error=0.17543445527553558\n",
      "88/100, error=0.1751587837934494\n",
      "89/100, error=0.17494070529937744\n",
      "90/100, error=0.17473207414150238\n",
      "91/100, error=0.17499515414237976\n",
      "92/100, error=0.1749383807182312\n",
      "93/100, error=0.17472536861896515\n",
      "94/100, error=0.1745220422744751\n",
      "95/100, error=0.1743277609348297\n",
      "96/100, error=0.1741420328617096\n",
      "97/100, error=0.17396438121795654\n",
      "98/100, error=0.1737942397594452\n",
      "99/100, error=0.17411363124847412\n",
      "100/100, error=0.17373131215572357\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ks_ZdbLQlHoL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
