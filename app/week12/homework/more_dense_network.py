"""
The implementation should include the functionalities needed for the previous week + all layers
should have option to have activation function (None, 'relu' or 'sigmoid'). Feedforward and
backpropagation should be working according to that. The optional task here (mainly for those,
who have done their previous hw with activation functions) is to implement backpropogation not
by gradient descent algorithm, but by Adam.
"""
import tensorflow as tf


class MoreDenseNetwork:
    def __init__(self):
        pass

    def call(self):
        pass

    def fit(self):
        pass
