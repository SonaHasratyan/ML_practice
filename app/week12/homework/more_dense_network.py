"""
The implementation should include the functionalities needed for the previous week + all layers
should have option to have activation function (None, 'relu' or 'sigmoid'). Feedforward and
backpropogation should be working according to that. The optional task here (mainly for those,
who have done their previous hw with activation functions) is to implement backpropogation not
by gradient descent algorithm, but by Adam.



"""